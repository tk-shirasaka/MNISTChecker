{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ゼロから作る Deep Learning\n",
    "\n",
    "## 概要\n",
    "\n",
    "これは書籍で学んだことをメモするためのノートです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 始めに\n",
    "\n",
    "必要なライブラリをインポートします\n",
    "\n",
    "- numpy (行列計算)\n",
    "- matplotlib.pyplot (グラフ描画)\n",
    "- PIL.Image (画像表示)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL.Image as Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 準備運動\n",
    "\n",
    "- numpyを使用してグラフの描画\n",
    "- matplotlib.pyplotを使用して画像の描画"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.arange(0, 6, 0.1)\n",
    "\n",
    "plt.plot(x, np.sin(x), label='sin')\n",
    "plt.plot(x, np.cos(x), label='cos')\n",
    "plt.xlabel('x')\n",
    "plt.xlabel('y')\n",
    "plt.title('sin & cos')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(Image.open('img/penguins.jpeg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## パーセプトロン\n",
    "\n",
    "入力値に応じて0か1を返す関数で以下の定義で表される\n",
    "\n",
    "- `n` : 入力の数\n",
    "\n",
    "$$\n",
    "y = \\begin{cases}\n",
    "  0 & x \\cdot w + b \\lt 0 \\\\\n",
    "  1 & x \\cdot w + b \\ge 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "出力値をグラフで表すと以下のような線になる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def step (x) :\n",
    "    return 0 if x < 0 else 1\n",
    "\n",
    "def total(x, w, b) :\n",
    "    return np.dot(x, w) + b\n",
    "\n",
    "def perseptron(x, w, b) :\n",
    "    return step(total(x, w, b))\n",
    "\n",
    "x = np.arange(0, 2, 0.001).reshape(200, 10)\n",
    "w = np.random.rand(10)\n",
    "b = -4\n",
    "plt.ylim(ymin=-0.2, ymax=1.2)\n",
    "plt.plot([perseptron(x, w, b) for x in x])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ニューラルネットワーク\n",
    "\n",
    "パーセプトロンの定義を以下のように書き換える\n",
    "\n",
    "$$\n",
    "y = h(x \\cdot w + b)\n",
    "$$\n",
    "\n",
    "$$\n",
    "h(a) = \\begin{cases}\n",
    "  0 & a \\lt 0 \\\\\n",
    "  1 & a \\ge 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "`h(x)`のことを__活性化関数__と呼ぶ\n",
    "\n",
    "ニューラルネットワークでは活性化関数に以下の関数を使用する\n",
    "\n",
    "- シグモイド関数\n",
    "- ReLU\n",
    "\n",
    "※ パーセプトロンで使用した関数は__ステップ関数__という\n",
    "\n",
    "### シグモイド関数\n",
    "\n",
    "$$\n",
    "h(x) = \\frac{1}{1 + \\mathrm{e}^{-x}}\n",
    "$$\n",
    "\n",
    "### ReLU\n",
    "\n",
    "$$\n",
    "h(a) = \\begin{cases}\n",
    "  0 & a \\lt 0 \\\\\n",
    "  a & a \\ge 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "シグモイド関数、ReLU、ステップ関数を比較すると以下のような線になる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sigmoid(x) :\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def relu(x) :\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def nuralnet(x, w, b, func) :\n",
    "    return func(total(x, w, b))\n",
    "\n",
    "plt.ylim(ymin=-0.2, ymax=1.2)\n",
    "plt.plot([nuralnet(x, w, b, sigmoid) for x in x], label='sigmoid')\n",
    "plt.plot([nuralnet(x, w, b, relu) for x in x], label='ReLU')\n",
    "plt.plot([perseptron(x, w, b) for x in x], label='perseptron', linestyle='--')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多層ニューラルネットワーク\n",
    "\n",
    "これまでは[入力]と[出力]だけの2層のネットワークを見てきたが、多層のネットワークも構築可能である\n",
    "\n",
    "層が増えれば複雑さが増すためより高度な分類が行える\n",
    "\n",
    "以下に4層ニューラルネットワークの例を示す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.arange(0, 2, 0.0001).reshape(200, 10, 10)\n",
    "w = [np.random.rand(10), np.random.rand(10), np.random.rand(1)]\n",
    "b = [-4, -7, -7]\n",
    "\n",
    "plt.plot([nuralnet(nuralnet(nuralnet(x, w[0], b[0], sigmoid), w[1], b[1], sigmoid), w[2], b[2], sigmoid) for x in x])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 出力関数\n",
    "\n",
    "最後の層で使用する活性関数のことを特別に出力関数という\n",
    "\n",
    "出力関数には以下の関数がある\n",
    "\n",
    "- 恒等関数\n",
    "- シグモイド関数\n",
    "- ソフトマックス関数\n",
    "\n",
    "### 恒等関数\n",
    "\n",
    "常に出力の値が入力値と等しくなる関数\n",
    "\n",
    "$$\n",
    "f(x) = x\n",
    "$$\n",
    "\n",
    "### ソフトマックス関数\n",
    "\n",
    "全ての出力層の合計値が1になるため、各ノードの出力値を確立として扱うことができる\n",
    "\n",
    "$$\n",
    "y_k = \\frac{\\mathrm{e}^{a_k}}{\\sum^n_{i=1} \\mathrm{e}^{a_i}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def softmax(x) :\n",
    "    exp = np.exp(x)\n",
    "    return exp / np.sum(exp)\n",
    "\n",
    "x = np.arange(0, 2, 0.0001).reshape(200, 10, 10)\n",
    "w = [np.random.rand(10), np.random.rand(10, 4), np.random.rand(4, 5)]\n",
    "b = [-4, -7, -1]\n",
    "\n",
    "plt.plot([nuralnet(nuralnet(nuralnet(x, w[0], b[0], sigmoid), w[1], b[1], sigmoid), w[2], b[2], softmax) for x in x])\n",
    "plt.title('Softmax')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
