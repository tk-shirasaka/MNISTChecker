{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ゼロから作る Deep Learning\n",
    "\n",
    "## 概要\n",
    "\n",
    "これは書籍で学んだことをメモするためのノートです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 始めに\n",
    "\n",
    "必要なライブラリをインポートします\n",
    "\n",
    "- numpy (行列計算)\n",
    "- matplotlib.pyplot (グラフ描画)\n",
    "- PIL.Image (画像表示)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL.Image as Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 準備運動\n",
    "\n",
    "- numpyを使用してグラフの描画\n",
    "- matplotlib.pyplotを使用して画像の描画"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.arange(0, 6, 0.1)\n",
    "\n",
    "plt.plot(x, np.sin(x), label='sin')\n",
    "plt.plot(x, np.cos(x), label='cos')\n",
    "plt.xlabel('x')\n",
    "plt.xlabel('y')\n",
    "plt.title('sin & cos')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(Image.open('img/penguins.jpeg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## パーセプトロン\n",
    "\n",
    "入力値に応じて0か1を返す関数で以下の定義で表される\n",
    "\n",
    "- `n` : 入力の数\n",
    "\n",
    "$$\n",
    "y = \\begin{cases}\n",
    "  0 & x \\cdot w + b \\lt 0 \\\\\n",
    "  1 & x \\cdot w + b \\ge 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "出力値をグラフで表すと以下のような線になる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def step (x) :\n",
    "    return 0 if x < 0 else 1\n",
    "\n",
    "def total(x, w, b) :\n",
    "    return np.dot(x, w) + b\n",
    "\n",
    "def perseptron(x, w, b) :\n",
    "    return step(total(x, w, b))\n",
    "\n",
    "x = np.arange(0, 2, 0.001).reshape(200, 10)\n",
    "w = np.random.rand(10)\n",
    "b = -4\n",
    "plt.ylim(ymin=-0.2, ymax=1.2)\n",
    "plt.plot([perseptron(x, w, b) for x in x])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ニューラルネットワーク\n",
    "\n",
    "パーセプトロンの定義を以下のように書き換える\n",
    "\n",
    "$$\n",
    "y = h(x \\cdot w + b)\n",
    "$$\n",
    "\n",
    "$$\n",
    "h(a) = \\begin{cases}\n",
    "  0 & a \\lt 0 \\\\\n",
    "  1 & a \\ge 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "`h(x)`のことを__活性化関数__と呼ぶ\n",
    "\n",
    "ニューラルネットワークでは活性化関数に以下の関数を使用する\n",
    "\n",
    "- シグモイド関数\n",
    "- ReLU\n",
    "\n",
    "※ パーセプトロンで使用した関数は__ステップ関数__という\n",
    "\n",
    "### シグモイド関数\n",
    "\n",
    "$$\n",
    "h(x) = \\frac{1}{1 + \\mathrm{e}^{-x}}\n",
    "$$\n",
    "\n",
    "### ReLU\n",
    "\n",
    "$$\n",
    "h(a) = \\begin{cases}\n",
    "  0 & a \\lt 0 \\\\\n",
    "  a & a \\ge 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "シグモイド関数、ReLU、ステップ関数を比較すると以下のような線になる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sigmoid(x) :\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def relu(x) :\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def neuralnet(x, w, b, func) :\n",
    "    return func(total(x, w, b))\n",
    "\n",
    "plt.ylim(ymin=-0.2, ymax=1.2)\n",
    "plt.plot([neuralnet(x, w, b, sigmoid) for x in x], label='sigmoid')\n",
    "plt.plot([neuralnet(x, w, b, relu) for x in x], label='ReLU')\n",
    "plt.plot([perseptron(x, w, b) for x in x], label='perseptron', linestyle='--')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多層ニューラルネットワーク\n",
    "\n",
    "これまでは[入力]と[出力]だけの2層のネットワークを見てきたが、多層のネットワークも構築可能である\n",
    "\n",
    "層が増えれば複雑さが増すためより高度な分類が行える\n",
    "\n",
    "以下に4層ニューラルネットワークの例を示す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.arange(0, 2, 0.0001).reshape(200, 10, 10)\n",
    "w = [np.random.rand(10), np.random.rand(10), np.random.rand(1)]\n",
    "b = [-4, -7, -7]\n",
    "\n",
    "plt.plot([neuralnet(neuralnet(neuralnet(x, w[0], b[0], sigmoid), w[1], b[1], sigmoid), w[2], b[2], sigmoid) for x in x])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 出力関数\n",
    "\n",
    "最後の層で使用する活性関数のことを特別に出力関数という\n",
    "\n",
    "出力関数には以下の関数がある\n",
    "\n",
    "- 恒等関数\n",
    "- シグモイド関数\n",
    "- ソフトマックス関数\n",
    "\n",
    "### 恒等関数\n",
    "\n",
    "常に出力の値が入力値と等しくなる関数\n",
    "\n",
    "$$\n",
    "y_k = a_k\n",
    "$$\n",
    "\n",
    "### ソフトマックス関数\n",
    "\n",
    "全ての出力層の合計値が1になるため、各ノードの出力値を確立として扱うことができる\n",
    "\n",
    "$$\n",
    "y_k = \\frac{\\mathrm{e}^{a_k}}{\\sum^n_{i=1} \\mathrm{e}^{a_i}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def softmax(x) :\n",
    "    exp = np.exp(x - np.max(x))\n",
    "    return exp / np.sum(exp)\n",
    "\n",
    "x = np.arange(0, 2, 0.0001).reshape(200, 10, 10)\n",
    "w = [np.random.rand(10), np.random.rand(10, 4), np.random.rand(4, 5)]\n",
    "b = [-4, -7, -1]\n",
    "\n",
    "plt.plot([neuralnet(neuralnet(neuralnet(x, w[0], b[0], sigmoid), w[1], b[1], sigmoid), w[2], b[2], softmax) for x in x])\n",
    "plt.title('Softmax')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 損失関数\n",
    "\n",
    "出力層で出力された値と実際の値の差異を表し、損失関数で得られたからどのように学習させるかを決めるための指標を得る\n",
    "\n",
    "具体的には以下のグラフが示すように損失関数の出力は曲線を描くため、微分を用いて傾きを求めることでどの方向に値を調整するかを把握することができる\n",
    "\n",
    "- 2乗和誤差\n",
    "- 交差エントロピー誤差\n",
    "\n",
    "ここでは実際の値(教師データ)を`t`とする\n",
    "\n",
    "### 2乗和誤差\n",
    "\n",
    "$$\n",
    "E = \\frac{1}{2} \\sum_k (y_k - t_k)^2\n",
    "$$\n",
    "\n",
    "### 交差エントロピー誤差\n",
    "\n",
    "$$\n",
    "E = - \\sum_k t_k \\log y_k\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def conv_2d(array) :\n",
    "    array[:] = np.array(array)\n",
    "    return array if array.ndim > 1 else np.reshape(array, (1, array.size))\n",
    "\n",
    "def mean_squared_error(y, t) :\n",
    "    return np.sum((conv_2d(y) - np.array(t)) ** 2, axis=1) / 2\n",
    "\n",
    "def cross_entropy_error(y, t) :\n",
    "    return -np.sum(np.array(t) * np.log(conv_2d(y) + 1e-7), axis=1)\n",
    "\n",
    "y = np.array([\n",
    "    np.zeros(200),\n",
    "    np.zeros(200),\n",
    "    np.zeros(200),\n",
    "    np.arange(1, 0, -0.005),\n",
    "    np.arange(0, 1, 0.005),\n",
    "]).T\n",
    "t = [0, 0 ,0 , 0, 1]\n",
    "\n",
    "plt.plot(mean_squared_error(y, t), label='mean squared')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.plot(cross_entropy_error(y, t), label='cross entropy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 微分\n",
    "\n",
    "微分は傾きが変化する関数の中で特定の時点での傾きを求める\n",
    "\n",
    "$$\n",
    "\\frac{df(x)}{dx} = \\lim_{h \\to 0} \\frac{f(x + h) - f(x)}{h}\n",
    "$$\n",
    "\n",
    "## 勾配\n",
    "\n",
    "1つの変数以外を定数と仮定して、その変数に対して微分することを__偏微分__といい、偏微分をすべての変数に対して行った結果をベクトルとしてまとめたものを勾配という\n",
    "\n",
    "$$\n",
    "\\left(\n",
    "  \\frac{\\partial f(x_0)}{\\partial x_0},\n",
    "  \\frac{\\partial f(x_1)}{\\partial x_1},\n",
    "  \\cdots\n",
    "  \\frac{\\partial f(x_n)}{\\partial x_n}\n",
    "\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def numerical_diff(f, x) :\n",
    "    h = 1e-4\n",
    "    return (f(x + h) - f(x - h)) / (2 * h)\n",
    "\n",
    "def numerical_gradient(f, x) :\n",
    "    h = 1e-4\n",
    "    x = conv_2d(x)\n",
    "    grad = np.zeros_like(x)\n",
    "    for i in range(x.shape[0]) :\n",
    "        for j in range(x.shape[1]) :\n",
    "            tmp = x[i][j]\n",
    "            x[i][j] = tmp + h\n",
    "            a = f(x[i])\n",
    "\n",
    "            x[i][j] = tmp - h\n",
    "            b = f(x[i])\n",
    "            grad[i][j] = (a - b) / (2 * h)\n",
    "\n",
    "            x[i][j] = tmp\n",
    "    return grad\n",
    "\n",
    "def test_func1(x) :\n",
    "    return 0.01 * x ** 2 + 0.1 * x\n",
    "\n",
    "def test_func2(x) :\n",
    "    return np.sum(conv_2d(x) ** 2, axis=1)\n",
    "\n",
    "x = np.arange(-50., 50., 1.)\n",
    "plt.plot(x, test_func1(x), label='y = 0.01x^2 + 0.1x')\n",
    "plt.plot(x, numerical_diff(test_func1, x), label='numerical diff', linestyle='--')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "x = np.meshgrid(np.arange(-10., 10., 1.), np.arange(-10., 10., 1.))\n",
    "x = [x[0].flatten(), x[1].flatten()]\n",
    "y = numerical_gradient(test_func2, np.array(x))\n",
    "plt.quiver(x[0], x[1], -y[0], -y[1], angles=\"xy\",color=\"#666666\")\n",
    "plt.title('gradient')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習率\n",
    "\n",
    "勾配を用いることである地点の傾きを得ることができる。\n",
    "\n",
    "傾きはパラメータを変化させたときにどのように出力値が変わるかを示すため、値を減らす方向にパラメータを動かすことで誤差を減らすことができる\n",
    "\n",
    "パラメータの変化量のことを__学習率(η)__という\n",
    "\n",
    "$$\n",
    "x_k = x_k - \\eta \\frac{\\partial f(x_k)}{\\partial x_k}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gradient_descent(f, x, lr=0.01, step=100) :\n",
    "    history = []\n",
    "    x = conv_2d(x)\n",
    "\n",
    "    for i in range(step) :\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "        history.append(x.copy())\n",
    "\n",
    "    return x, np.array(history).T\n",
    "\n",
    "x, history = gradient_descent(test_func2, np.array([-3.0, 4.0]), lr=0.1)\n",
    "\n",
    "plt.plot(history[0], history[1], 'o')\n",
    "plt.ylim(ymin=-5, ymax=5)\n",
    "plt.xlim(xmin=-5, xmax=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ニューラルネットワークの実装\n",
    "\n",
    "ニューラルネットワークで学習をするためにどのように処理を進めていくかをいかに示します\n",
    "\n",
    "1. 重み(w)の初期値と入力値を決める\n",
    "2. ニューラルネットワークの各レイヤーに対して活性関数を適用する\n",
    "  - 出力層 : ソフトマックス関数\n",
    "  - 出力層以外 : シグモイド関数\n",
    "3. 出力値に対して損失関数を適用する\n",
    "4. [2] - [3]をすべてのパラメータに対して適用し勾配を求める\n",
    "5. パラメータを更新して[2] - [4]を繰り返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def init_network(shape) :\n",
    "    i = 0\n",
    "    w = []\n",
    "    b = []\n",
    "\n",
    "    for curr in shape :\n",
    "        if i > 0 :\n",
    "            w.append(np.random.rand(prev, curr))\n",
    "            b.append(np.random.rand(curr))\n",
    "\n",
    "        prev = curr\n",
    "        i += 1\n",
    "\n",
    "    return w, b\n",
    "\n",
    "def gradient_network(x, w, b, t, step=100, lr=0.01) :\n",
    "    history = []\n",
    "    loss_func = lambda arg : loss(x, w, b, t)\n",
    "\n",
    "    for i in range(step) :\n",
    "        grad = []\n",
    "\n",
    "        for (_w, _b) in zip(w, b) :\n",
    "            grad.append((numerical_gradient(loss_func, _w), numerical_gradient(loss_func, _b)))\n",
    "\n",
    "        for (_w, _b, _grad) in zip(w, b, grad) :\n",
    "            _w -= lr * _grad[0]\n",
    "            _b -= lr * _grad[1][0]\n",
    "        history.append(loss(x, w, b, t))\n",
    "\n",
    "    return history\n",
    "\n",
    "def loss(x, w, b, t) :\n",
    "    for (_w, _b) in zip(w[:-1], b[:-1]) :\n",
    "        x = neuralnet(x, _w, _b, sigmoid)\n",
    "\n",
    "    y = neuralnet(x, w[-1], b[-1], softmax)\n",
    "    return cross_entropy_error(y, t)\n",
    "\n",
    "x = np.random.rand(4)\n",
    "w, b = init_network((4, 2, 3, 5))\n",
    "t = [0, 0, 0, 1, 0]\n",
    "history = gradient_network(x, w, b, t, step=500)\n",
    "\n",
    "plt.plot(history, label='cross entropy error')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 誤差逆伝搬法\n",
    "\n",
    "微分を単純な計算の集合として考えることで、計算を簡略化する\n",
    "\n",
    "$$\n",
    "y = ab + c\n",
    "$$\n",
    "\n",
    "このような関数があった場合、この関数を以下のように分解する\n",
    "\n",
    "$$\n",
    "y = A + c \\\\\n",
    "A = ab\n",
    "$$\n",
    "\n",
    "この関数内のすべての微分は以下のようになる\n",
    "\n",
    "$$\n",
    "\\frac{dA}{da} = b \\\\\n",
    "\\frac{dA}{db} = a \\\\\n",
    "\\frac{dy}{dA} = 1 \\\\\n",
    "\\frac{dy}{dc} = 1\n",
    "$$\n",
    "\n",
    "それぞれの変数に対する微分を示す\n",
    "\n",
    "- aに対する微分\n",
    "$$\n",
    "\\frac{dy}{da} = \\frac{dA}{da} \\frac{dy}{dA} = b \\times 1 = b\n",
    "$$\n",
    "\n",
    "- bに対する微分\n",
    "$$\n",
    "\\frac{dy}{db} = \\frac{dA}{db} \\frac{dy}{dA} = a \\times 1 = a\n",
    "$$\n",
    "\n",
    "- cに対する微分\n",
    "$$\n",
    "\\frac{dy}{dc} = 1\n",
    "$$\n",
    "\n",
    "このように微分を計算する際は逆順に算出するため__逆伝搬__という\n",
    "\n",
    "### 加算の場合\n",
    "\n",
    "$$\n",
    "y = a + b\n",
    "$$\n",
    "\n",
    "上の計算を考える場合、a、b両方に対する微分の結果は1になる\n",
    "\n",
    "### 乗算の場合\n",
    "\n",
    "$$\n",
    "y = ab\n",
    "$$\n",
    "\n",
    "上の計算を考える場合、aに対する微分はbに、bに対する微分はaになることから入力値を反転させた値が微分の結果になる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 活性関数の逆伝搬\n",
    "\n",
    "### ReLU\n",
    "\n",
    "$$\n",
    "y = \\begin{cases}\n",
    "  0 & x \\lt 0 \\\\\n",
    "  x & x \\ge 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "上記の定義からReLUの微分は\n",
    "\n",
    "$$\n",
    "\\frac{dy}{dx} = \\begin{cases}\n",
    "  0 & x \\lt 0 \\\\\n",
    "  1 & x \\ge 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "となるため、xが負数となる場合は以降の逆伝搬の算出は不要となり、計算量を軽減することができる\n",
    "\n",
    "### シグモイド関数\n",
    "\n",
    "$$\n",
    "y = \\frac{1}{1 + \\mathrm{e}^{-x}}\n",
    "$$\n",
    "\n",
    "を分解すると\n",
    "\n",
    "$$\n",
    "y = \\frac{1}{A} \\\\\n",
    "A = 1 + B \\\\\n",
    "B = \\mathrm{e}^{C} \\\\\n",
    "C = -x\n",
    "$$\n",
    "\n",
    "となり、それぞれの微分は\n",
    "\n",
    "$$\n",
    "\\frac{dC}{dx} = -1 \\\\\n",
    "\\frac{dB}{dC} = \\mathrm{e}^{C} \\\\\n",
    "\\frac{dA}{dB} = 1 \\\\\n",
    "\\frac{dy}{dA} = -\\frac{1}{A^2} = -y^2\n",
    "$$\n",
    "\n",
    "である。これを計算すると\n",
    "\n",
    "$$\n",
    "\\frac{dy}{dx} = y^2\\mathrm{e}^{-x}\n",
    "$$\n",
    "\n",
    "となり微分の結果、入力値`x`と出力値`y`から算出可能であるため途中の計算を省略できる\n",
    "\n",
    "更に、`y`は`x`から導出されるため`y`のみから算出できる。\n",
    "\n",
    "$$\n",
    "y^2\\mathrm{e}^{-x} = y(1 - y)\n",
    "$$\n",
    "\n",
    "__※ シグモイド関数で`x`の低を`e`にしていた理由は`e`が微分しても`e`になるという性質を持っていたため計算が容易になるという理由だと思われる__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ReLU :\n",
    "    def __init__(self) :\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x) :\n",
    "        self.out = relu(x)\n",
    "        return self.out\n",
    "\n",
    "    def backword(self, dout) :\n",
    "        return (self.out > 0) * dout\n",
    "\n",
    "class Sigmoid :\n",
    "    def __init__(self) :\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, x) :\n",
    "        self.out = sigmoid(x)\n",
    "        return self.out\n",
    "\n",
    "    def backword(self, dout) :\n",
    "        return dout * self.out * (1. - self.out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 入力値の逆伝搬\n",
    "\n",
    "ニューラルネットワークのパラメータは\n",
    "\n",
    "$$\n",
    "y = x \\cdot w + b\n",
    "$$\n",
    "\n",
    "で計算されるため、これを分解すると\n",
    "\n",
    "$$\n",
    "y = A + b \\\\\n",
    "A = x \\cdot w\n",
    "$$\n",
    "\n",
    "となり、それぞれの微分は\n",
    "\n",
    "$$\n",
    "\\frac{dA}{dx} = w^T \\\\\n",
    "\\frac{dy}{dA} = 1\n",
    "$$\n",
    "\n",
    "となり、これを計算すると\n",
    "\n",
    "$$\n",
    "\\frac{dy}{dx} = w^T\n",
    "$$\n",
    "\n",
    "で表すことができる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Params:\n",
    "    def __init__(self) :\n",
    "        self.w = None\n",
    "\n",
    "    def forward(self, x, w, b) :\n",
    "        self.w = w\n",
    "        return total(x, w, b)\n",
    "\n",
    "    def backword(self, dout) :\n",
    "        return np.dot(dout, self.w.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
